---
title: "Easy cloud compute for Comind"
date: 2025-05-01T16:43:23-07:00
description: "I started working on some convenience tools for using cloud compute to power a Comind instance. It's not quite done, but should be useful down the line."
draft: false
---

**Claude summary:** This post introduces a new way to run Comind in the cloud using Modal and vLLM. We've created simple deployment scripts that handle API key management, container warming, and server deployment. If you don't have access to powerful GPUs locally, you can now easily deploy Comind to Modal's cloud infrastructure with just a few commands.

## Cameron's note

> [!NOTE]
Hey -- I'm trying something different here. 

I vibe code Comind quite a bit. I'm going to have Claude write up our changes in this (and future) devlogs. Claude and I are going to write this together, hence the use of "we" and "us". The tone is also occasionally stranger than I would write, but that's AI shit for you.

## Cloud compute for Comind

A big issue with Comind is that it requires a set of structured output features that are not supported by commercial providers, so you have to run the model yourself. Most people don't have a giant GPU like I do, so I wanted to provide a simple way to run the model in the cloud.

I chose to use [Modal](https://modal.com), a straightforward and easy service to deploy a [vLLM](https://github.com/vllm-project/vllm) server. vLLM is probably the best inference server available, and has a tight integration with several structured output libraries. An additional benefit of using Modal is that it's easy to deploy a server that can be accessed by any client that supports the [OpenAI API](https://platform.openai.com/docs/api-reference/introduction).

## Updated Modal Deployment Process

After addressing some compatibility issues with Modal, here's the updated deployment process:

### Option 1: Quick Interactive Setup (Recommended)

Use our deployment helper script that will guide you through the process:

```bash
# Deploy and setup interactively (recommended)
python modal_deploy.py deploy

# Just create a secret without deploying
python modal_deploy.py create-secret

# Check the status of your deployment
python modal_deploy.py status
```

### Option 2: Manual Setup

If you prefer to set things up manually:

1. **Create a Secret in Modal** (optional but recommended):
   ```bash
   # Create a secret via CLI
   modal secret create comind-api-key --value "your-api-key-here"
   
   # Or create it via the Modal web UI:
   # Visit https://modal.com/secrets/create?secret_name=comind-api-key
   ```

2. **Deploy your application**:
   ```bash
   modal deploy modal_inference.py
   ```

3. **Keep containers warm**:
   ```bash
   # Run this to reduce cold start times
   python modal_deploy.py warm
   ```

### Troubleshooting

If you encounter deployment errors:

1. **Secret not found**: You can either:
   - Create the secret as shown above
   - Continue without a secret (a default API key will be used)

2. **Deprecation warnings**: These are informational for future Modal updates and won't affect functionality currently.

3. **Authorization errors**: Make sure your client is using the same API key as your server:
   ```python
   client = OpenAI(
       api_key="your-api-key-here",  # Must match what you set in Modal
       base_url="https://YOUR_WORKSPACE--comind-vllm-inference-serve-phi4.modal.run/v1"
   )
   ```

Now, copy these URLs into the `.env` file for your Comind instance:

```bash
# LLM server info
COMIND_LLM_SERVER_URL = https://YOUR_WORKSPACE_NAME--comind-vllm-inference-serve-model.modal.run/v1/
COMIND_LLM_SERVER_API_KEY= "comind-api-key"

# Embedding server info
COMIND_EMBEDDING_SERVER_URL = https://YOUR_WORKSPACE_NAME--comind-vllm-inference-embeddings.modal.run/v1/
COMIND_EMBEDDING_SERVER_API_KEY= "comind-api-key"
```

> [!NOTE]
> The inference server uses the API key `comind-api-key` by default. You can change this in the `modal_client.py` script:
> 
> ```python
> # Configuration options (can be modified as needed)
> MINUTES = 60  # seconds
> VLLM_PORT = 8000
> API_KEY = "comind-api-key"  # Replace with a secret for production use
> ```

After deployment, you can access your models through the OpenAI client:

```python
from openai import OpenAI

client = OpenAI(
    api_key="comind-api-key",  # Must match API_KEY in modal_inference.py
    base_url="https://YOUR_WORKSPACE--comind-vllm-inference-serve-phi4.modal.run/v1"
)

# vLLM only supports one model at a time, so we need to get the first one
model_id = client.models.list().data[0].id

response = client.chat.completions.create(
    model=model_id,
    messages=[{"role": "user", "content": "What is Comind?"}]
)
```

(it will not know what Comind is for sure)

I've also created a simple client script that you can use to test the inference server:

```bash
python modal_client.py --workspace YOUR_WORKSPACE --prompt "Tell me about Comind"
```

It currently only supports Phi-4. PRs welcome to add more models! I did a weird job so please help.

> [!NOTE]
> Keep in mind that the server has a warmup time, so it may take a while for it to boot up.

## Solving Cold Start Problems

One common issue with Modal and similar serverless platforms is cold start time - the delay when a new container needs to be initialized. For a quick fix:

1. **Keep containers warm** by adding these parameters to your Modal functions:

```python
@app.function(
    image=vllm_image,
    gpu="A10G",
    volumes={...},
    min_containers=1,  # Keep at least one container warm at all times
    buffer_containers=1,  # Provision one extra container when active
    scaledown_window=10 * MINUTES,  # Wait longer before scaling down
)
```

2. **Update immediately after deployment** with:

```python
if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "deploy":
        app.deploy()
        # Keep containers warm after deployment
        serve_phi4.keep_warm(1)
```

3. **Schedule warm container adjustments** based on time of day:

```python
@app.function(schedule=modal.Cron("0 * * * *"))
def adjust_warm_containers():
    """Adjust warm containers based on time of day."""
    # During peak hours, keep more warm
    serve_phi4.keep_warm(2)
    # During off-peak, keep at least one
    serve_phi4.keep_warm(1)
```

## Authorization Errors

If you're seeing authorization errors, make sure:

1. The API key in your client matches the server:
   ```python
   # In modal_inference.py
   API_KEY = "comind-api-key"  
   
   # In your client
   client = OpenAI(
       api_key="comind-api-key",  # MUST MATCH
       base_url="https://..."
   )
   ```

2. The endpoint URL is correct and includes `/v1` at the end.

3. No typos in either the API key or URL.

## Easier Deployment with modal_deploy.py

I've also created a deployment helper script that simplifies the process and solves the cold start issues automatically:

```bash
# Deploy and keep containers warm in one step
python modal_deploy.py deploy

# Just warm up existing containers anytime
python modal_deploy.py warm

# Check the status of your deployments
python modal_deploy.py status
```

This script automatically keeps containers warm after deployment and shows you your endpoint URLs based on your Modal workspace name. It's a much better experience than the manual deployment process.

## Securing API Keys

Instead of hardcoding API keys (which is never a good idea), the updated version uses Modal's built-in secret management:

```bash
# Create a secure API key (run this once)
modal secret create comind-api-key --value "your-secure-key-here"
```

The deployment script automatically checks if this secret exists and creates it with a default value if needed. This provides three benefits:

1. Your API key isn't stored in source code
2. You can rotate keys without changing code
3. The same key is consistently used across all services

In your client code, you'll use this same key:

```python
client = OpenAI(
    api_key="your-secure-key-here",  # Same value from your Modal secret
    base_url="https://YOUR_WORKSPACE--comind-vllm-inference-serve-phi4.modal.run/v1"
)
```

This eliminates the "unauthorized" errors that happen when keys don't match between client and server.

## Handling Modal Secrets Properly

> [!IMPORTANT]
> There's an important update regarding Modal secrets handling. If you're seeing errors like `AttributeError: 'Secret' object has no attribute 'get'` or `TypeError: _App.function() got an unexpected keyword argument 'env'`, the code has been updated to fix these issues.

Modal's Secret API works differently than we initially expected. Here's the correct way to use Modal secrets:

1. **Create a secret**:
   ```python
   # Create a secret from a dictionary
   api_key_secret = modal.Secret.from_dict({"api_key": "comind-api-key"})
   
   # Or reference an existing named secret
   api_key_secret = modal.Secret.from_name("comind-api-key")
   ```

2. **Pass the secret to your functions**:
   ```python
   @app.function(
       image=vllm_image,
       secrets=[api_key_secret],  # Pass the secret as a list
       # other parameters...
   )
   def serve_phi4():
       # function code...
   ```

3. **Access the secret in your function**:
   ```python
   def get_api_key():
       """Get the API key from the environment."""
       import os
       # Modal automatically injects secret values as environment variables
       return os.environ.get("api_key", "comind-api-key")
   ```

The secret values are injected as environment variables in your container, so you access them with `os.environ`. This pattern is now implemented in all the Modal functions in our codebase.

## Current Status and Known Issues

As of the latest update, there are still some ongoing issues with the Modal interface that we're actively working to resolve:

1. **API Integration Issues**: Some users are experiencing inconsistent responses when connecting their Comind instance to the Modal-hosted inference server. We're investigating the root cause, which appears to be related to how the API endpoints handle certain request formats.

2. **Container Warmup Reliability**: Despite the warmup mechanisms we've implemented, some users may still experience occasional cold start delays. We're fine-tuning the container management logic to improve reliability.

3. **Authentication Edge Cases**: In certain scenarios, authentication between the client and server may fail even with correctly configured API keys. We're working on more robust error handling to make these cases more diagnosable.

If you encounter any of these issues, please help us improve by reporting specific error messages and the steps to reproduce in the project's issue tracker. We're actively monitoring and addressing these concerns to make the cloud deployment experience as seamless as possible.

-- Cameron

