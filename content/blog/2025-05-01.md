---
title: "Comind development update: Modal inference and conceptualization"
date: 2025-05-01T16:43:23-07:00
description: "Recent progress on Comind's backend infrastructure and semantic processing capabilities."
draft: false
---

## Cameron's note

> [!NOTE]
> Hey -- I'm trying something different here. 
> 
> I vibe code Comind quite a bit. I'm going to have Claude write up our changes in this devlog. Claude and I are going to write this together, hence the use of "we" and "us". The tone is also occasionally stranger than I would write, but that's AI shit for you.
>
> So -- here goes. 

## Running LLMs in the Cloud for Comind

One of the biggest challenges with Comind has been accessibility. Comind requires structured output features not supported by commercial providers, forcing users to run models themselves. This creates a barrier for anyone without access to powerful GPUs.

Our goal was simple: create a way for Comind users to run inference in the cloud without needing local GPU hardware.

**Cameron** IMPORTANT: The Modal implementation described below is still in early development and not yet ready for production use. There are several configuration issues that need to be resolved. Please consider this a preview of future functionality rather than something you should be running right now.

## The Modal + vLLM Solution

We chose [Modal](https://modal.com) as our cloud platform for several reasons:

1. It provides easy access to GPU resources
2. It has a straightforward deployment model
3. It integrates well with [vLLM](https://github.com/vllm-project/vllm) - one of the best inference servers available

This combination gives us an OpenAI-compatible API that works with the existing Comind infrastructure while providing the structured output capabilities we need.

## What We've Built

We've created three core components:

1. `modal_inference.py` - The main inference server supporting multiple models
2. `modal_client.py` - A test client for validating the deployment
3. `modal_deploy.py` - A helper script that simplifies deployment and management

The system supports several models with different resource requirements:

- **Phi-4** (A10G GPU) - Microsoft's flagship 4B parameter model
- **Hermes-3-Llama-3.1-8B** (A10G GPU) - NousResearch's 8B parameter model
- **Hermes-3-Llama-3.2-3B** (T4 GPU) - NousResearch's 3B parameter model
- **Phi-3-mini** (T4 GPU) - Smaller Microsoft model for less powerful GPUs
- **TinyLlama-1.1B** (T4 GPU) - Ultra-lightweight model
- **Qwen3-0.6B** (T4 GPU) - RedHat's efficient 0.6B model
- **mxbai-embed-xsmall** (T4 GPU) - Efficient text embedding model

**Cameron** However -- there's still some issues here, particularly with choosing GPUs. T4s are cheap but too old, and need some additional TLC to get them to work. Something something `--dtype=half` or something. I'll figure it out.

## Deployment Options

### Interactive Deployment (Recommended)

The simplest approach is using our deployment helper:

```bash
# Deploy with interactive setup
python modal_deploy.py deploy

# Create a secret without deploying
python modal_deploy.py create-secret

# Check deployment status
python modal_deploy.py status
```

This guides you through selecting models to deploy and handles container warming automatically.

### Manual Deployment

For those who prefer more control:

1. **Create a Secret** (recommended):
   ```bash
   modal secret create comind-api-key --value "your-api-key-here"
   ```

2. **Deploy the application**:
   ```bash
   modal deploy modal_inference.py
   ```

3. **Keep containers warm**:
   ```bash
   python modal_deploy.py warm
   ```

## Key Challenges We Solved

During development, we encountered several significant challenges:

### 1. Cold Start Times

Modal, like most serverless platforms, suffers from "cold start" delays when initializing new containers. This was particularly problematic for LLM inference where users expect quick responses.

We implemented several strategies to address this:

```python
@app.function(
    # Other parameters...
    min_containers=1,  # Keep at least one container warm
    buffer_containers=1,  # Provision extra container when active
    scaledown_window=10 * MINUTES,  # Delay scaling down
)
```

The `modal_deploy.py` script also immediately warms containers after deployment:

```python
# Keep containers warm after deployment
serve_phi4.keep_warm(1)
```

### 2. Authentication & Security

Initially, we hardcoded API keys, which created inconsistencies between client and server configurations. We switched to Modal's secret management system:

```python
# Create a secure API key
modal secret create comind-api-key --value "your-secure-key-here"
```

Our code now checks for existing secrets and creates them when needed:

```python
# Try to use a secret if it exists
try:
    api_key_secret = modal.Secret.from_name("comind-api-key")
    has_secret = True
except:
    print("No 'comind-api-key' secret found. Using default API key.")
    has_secret = False
```

### 3. Modal Secret Handling

We discovered that Modal's Secret API works differently than expected. After some trial and error, we implemented the correct pattern:

1. **Reference existing secrets**:
   ```python
   api_key_secret = modal.Secret.from_name("comind-api-key")
   ```

2. **Pass secrets to functions**:
   ```python
   @app.function(
       # Other parameters...
       secrets=[api_key_secret],
   )
   ```

3. **Access via environment variables**:
   ```python
   def get_api_key():
       import os
       return os.environ.get("api_key", "comind-api-key")
   ```

This approach fixed the errors users were seeing like `AttributeError: 'Secret' object has no attribute 'get'`.

## Using the API

After deployment, you'll have OpenAI-compatible endpoints for each model:

```python
from openai import OpenAI

client = OpenAI(
    api_key="comind-api-key",  # Must match the server configuration
    base_url="https://YOUR_WORKSPACE--comind-vllm-inference-serve-phi4.modal.run/v1"
)

response = client.chat.completions.create(
    model="microsoft/Phi-4",
    messages=[{"role": "user", "content": "Hello, how are you?"}]
)
```

For testing, you can use the included client script:

```bash
python modal_client.py --workspace YOUR_WORKSPACE --prompt "Tell me about Comind"
```

## Configuring Comind

To connect your Comind instance to the Modal servers, update your `.env` file:

```bash
# LLM server info
COMIND_LLM_SERVER_URL = https://YOUR_WORKSPACE_NAME--comind-vllm-inference-serve-model.modal.run/v1/
COMIND_LLM_SERVER_API_KEY= "comind-api-key"

# Embedding server info
COMIND_EMBEDDING_SERVER_URL = https://YOUR_WORKSPACE_NAME--comind-vllm-inference-embeddings.modal.run/v1/
COMIND_EMBEDDING_SERVER_API_KEY= "comind-api-key"
```

## What conceptualization looks like

Conceptualization is the process of Comind examining a piece of content and generating a list of concepts and relationships between them. 

All agents on Comind can be "associated" with a sphere, meaning that they will take on a common system prompt describing what the sphere is supposed to do.

This system prompt is called the "core perspective" of the sphere, and governs the behavior of the entire sphere.

You can define the core perspective however you want -- you could make a sphere that will continuously think about 

- Don Cheadle
- the Comind project
- the semantic web
- the void
- antiques (this one was funny)

**Cameron** I created a new sphere called "comind" that is responsible for understanding the Comind project. It is associated with the following system prompt:

```
Your role is to understand and build the comind network, including:

- introspect on your core functions
- design improvements or system
modifications
- comment/observe the network at a high
level
```

Defining the prompt this way gives the sphere a specific "voice" that it uses to understand the content. 

For example, the prompt above uses a lot of technical-sounding language like "core functions" and "system modifications". It also refers to the network as a whole, which implies some kind of higher-level goal or purpose.  

Here's an example output:

```
conceptualizer - INFO - accessibility - PART_OF - The content discusses generating alt text for images, which is a key aspect of web accessibility. - 0.8
conceptualizer - INFO - ai integration - SUPPORTS - The use of Google Gemini AI to generate alt text illustrates integration of AI in practical applications. - 0.75
conceptualizer - INFO - firefox extension - INSTANCE_OF - The specific product discussed is a Firefox extension, representing a tangible instance of technology application. - 0.9
conceptualizer - INFO - semantic web - PART_OF - Improving accessibility and integrating AI ties into broader semantic web goals by enhancing information usability. - 0.65
conceptualizer - INFO - open source collaboration - PART_OF - Add-ons are typically open source, hinting at collaboration in development and improvement. - 0.6
conceptualizer - INFO - network enhancement - CONTRADICTS - While not explicitly mentioned, enhancing tools like alt text generators can indirectly support network understanding by improving content accessibility. - 0.5
conceptualizer - INFO - real time feedback - INSTANCE_OF - The author's real-time updates about version changes and the availability of the new version exemplify real-time feedback mechanisms. - 0.7
```

The "comind" sphere utilizes its resources to understand the Comind project, generating concepts that connect to the core perspective. Each line shows:

1. A concept identified in the content
2. The relationship type (PART_OF, SUPPORTS, INSTANCE_OF, etc.)
3. An explanation of how the concept relates to the content
4. A confidence score between 0 and 1

**Cameron** This is a very simple example, but it shows the potential of the system. More complex spheres could be made by writing a system prompt that describes the sphere's role in more detail.

## Current Status and Next Steps

While the system is functional, we're still addressing some issues:

1. **API Integration Issues**: Some users experience inconsistent responses between Comind and the Modal-hosted servers
2. **Container Warmup Reliability**: Occasional cold start delays despite our mitigation strategies
3. **Authentication Edge Cases**: Rare authentication failures even with correct configurations

**Claude** We're actively working to improve these areas.
**Cameron** Thanks for reading, buddy.

